{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #  **Topic modeling fine tuning**\n",
    "\n",
    "---\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**D**entro del minado de datos existe la tarea de identificar un conjunto de palabras que representen al texto de un `documento` y en esta libreta se mostrara la solucion llamada *Topic Modeling*, el cual utiliza un enfoque probabilistico para resolver esta problematica.\n",
    "\n",
    "El curso de la libreta es el siguiente:\n",
    " - Vectorizacion\n",
    " - *Ingenieria de caracteristicas*\n",
    " - Aplicar Topic modeling\n",
    " - Validacion \"*empirica*\"\n",
    " \n",
    "\n",
    "\n",
    "Los boletines expedidos por la [pagina](https://presidente.gob.mx/) del presidente mexicano Andres Manuel Lopez Obrador forman el `corpus` usado a lo largo del documento, el cual fue procesado y limpiado en otra libreta. \n",
    "\n",
    "El EDA se realizara en la plataforma [Tableau](https://www.tableau.com/es-mx), para esto se guardaran periodicamente algunos valores en un archivo con extencion '.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Titulo       Fecha  \\\n",
      "2255  Poderes Ejecutivo y Judicial coordinan esfuerz...  13.02.2020   \n",
      "2692  Presidente analiza terminar construcción de 54...  27.07.2019   \n",
      "1929  Fotogalería – Presidente inaugura Unidad de Me...  31.08.2020   \n",
      "2291  Avanza adhesión de estados al Instituto de Sal...  28.01.2020   \n",
      "1184  Reforma eléctrica fortalecerá a CFE y proteger...  23.10.2021   \n",
      "1436  México recibirá un millón de dosis de vacuna J...  08.06.2021   \n",
      "918   Presidente presentará iniciativa de reforma el...  31.03.2022   \n",
      "3011  Presidente encabeza presentación de iniciativa...  02.02.2019   \n",
      "\n",
      "                                              Titulo_ns  \n",
      "2255  Poderes Ejecutivo Judicial coordinan esfuerzos...  \n",
      "2692  Presidente analiza terminar construcción 54 un...  \n",
      "1929  Fotogalería – Presidente inaugura Unidad Medic...  \n",
      "2291          Avanza adhesión Instituto Salud Bienestar  \n",
      "1184  Reforma eléctrica fortalecerá CFE protegerá bi...  \n",
      "1436  México recibirá millón dosis vacuna Johnson & ...  \n",
      "918   Presidente presentará iniciativa reforma elect...  \n",
      "3011  Presidente encabeza presentación iniciativa ge...  \n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_json('../primera_iteracion/juve_datos.json',orient='record')\n",
    "\n",
    "#############################################################################\n",
    "# Nota, este codigo esta por mientras el tokenizado. Borrar esto despues...\n",
    "stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "def remove_stopwords(text):\n",
    "    texto = text.split()\n",
    "    text = [word for word in texto if word not in stopword_es]\n",
    "    text_r = ' '.join(text)\n",
    "    return text_r\n",
    "corpus['Titulo_ns'] = corpus['Titulo'].apply(lambda x: remove_stopwords(x))\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "corpus_validacion = corpus.sample(frac=8 / len(corpus) , random_state = 564)\n",
    "corpus_entrenamiento = corpus.drop(corpus_validacion.index)\n",
    "\n",
    "print(corpus_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**H**ay que recalcar que los documentos no estan etiquetados, por lo que para el *topic modeling* se esta restringido a **algoritmos no supervisados**. Al algoritmo que se utiliza para conseguir la clasificacion de los topicos se le llamara **modelo**. El corpus de validacion es para realizar una evaluacion empirica de nuestro resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizacion\n",
    "---\n",
    "\n",
    "**E**l formato de texto suele ser poco amigable para los algoritmos por lo que es conveniente transformar el texto a un formato numerico que nos permita representar las **caracteristicas numericas** del texto. A esto se le llama vectorizado.\n",
    "\n",
    "La eleccion de las caracteristicas numericas depende en gran medida del modelo que se este usando. Los vectorizadores mas comunes en PLN son los basados en **frecuencias** y los basados en **algoritmos de aprendizaje**, los primeros nos permiten representar un documento por la frecuencia de sus palabras y se suele decir que los segundos logran capturar el lenguaje semantico de las palabras.    \n",
    "\n",
    "Intuitivamente se puede pensar que las palabras mas recurrentes representan a un documento, pero esto puede no ser verdad en los casos extremos donde una palabra aparece mucho en el corpus. Para resolver este problema existe el vectorizado *term frequency - inverse document frequency* `(tfidf)` .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias_del_corpus = TfidfVectorizer(\n",
    "    ngram_range = (1,1),\n",
    "    max_df=0.99999999999, \n",
    "    min_df=0\n",
    "    )\n",
    "\n",
    "corpus_vectorizado = frecuencias_del_corpus.fit_transform(corpus_entrenamiento.Titulo_ns) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando frecuencias \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def exportar_a_csv(text):\n",
    "    texto = text.split()\n",
    "    text = [word for word in texto if word not in stopword_es]\n",
    "    text_r = ' '.join(text)\n",
    "    return text_r\n",
    "\n",
    "print(frecuencias_del_corpus.stop_words_)\n",
    "\n",
    "print(frecuencias_del_corpus.transform(['tren']))\n",
    "\n",
    "#for token in frecuencias_del_corpus.get_feature_names_out():\n",
    "#    print(token)\n",
    "    \n",
    "\n",
    "'''\n",
    "with open('palabras_por_documento.csv', 'w', encoding='utf-8') as palabras:\n",
    "    palabras.write('id , palabra , fecha\\n')\n",
    "    c = 0\n",
    "\n",
    "    for registro in df_articulos.iloc:\n",
    "        fecha = registro[1]\n",
    "        for palabra in registro[2].split():\n",
    "            aux = '' + str(c) + ',' + palabra + ','+fecha+'\\n'\n",
    "            palabras.write(aux)\n",
    "            c= c+1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingenieria de caracteristicas\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando nuevas caracteristicas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27056\\1631263819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Estos parametros son los que debe mover el equipo de Modelado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m76\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_fit' is not defined"
     ]
    }
   ],
   "source": [
    "# Existen varias formas de descomponer, al parecer el LDA es el mas 'aca'\n",
    "#Estos parametros son los que debe mover el equipo de Modelado\n",
    "lda = LatentDirichletAllocation(n_components = 32, random_state = 76)\n",
    "lda.fit(tfidf_fit)\n",
    "\n",
    "feature = tfidf.get_feature_names()\n",
    "for ind, topic in enumerate(lda.components_):\n",
    "    print('Top 50 words in topic {}'.format(ind))\n",
    "    print('-'*25)\n",
    "    topic_ordenado = topic.argsort()[::-1]\n",
    "    top_50 = topic_ordenado[:50]\n",
    "    print([feature[i] for i in top_50] , '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando entropia del topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion empirica\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLN_topic_modeling",
   "language": "python",
   "name": "topic_modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
