{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "6sxl1cy0tZ"
      },
      "source": [
        "> #  **Topic modeling fine tuning**\n",
        "\n",
        "---\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FePojpJpxN"
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "#from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "#from sklearn.model_selection import ShuffleSplit\n",
        "#from sklearn.feature_selection import chi2\n",
        "#from sklearn.decomposition import PCA\n",
        "#\n",
        "#import numpy as np\n",
        "#import pandas as pd\n",
        "#import random\n",
        "#import nltk\n",
        "import matplotlib.pyplot as plt \n",
        "    \n",
        "x = [1,2,3] \n",
        "y = [2,4,1] \n",
        "plt.plot(x, y) \n",
        "plt.xlabel('x - axis') \n",
        "plt.ylabel('y - axis') \n",
        "plt.title('My first graph!') \n",
        "plt.show() "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "sAmqdzSm2Z"
      },
      "source": [
        "---\n",
        "**D**entro del minado de datos existe la tarea de identificar un conjunto de palabras que representen al texto de un `documento` y en esta libreta se mostrara la solucion llamada *Topic Modeling*, el cual utiliza un enfoque probabilistico para resolver esta problematica.\n",
        "\n",
        "El curso de la libreta es el siguiente:\n",
        " - Vectorizacion\n",
        " - *Ingenieria de caracteristicas*\n",
        " - Aplicar Topic modeling\n",
        " - Validacion \"*empirica*\"\n",
        " \n",
        "\n",
        "\n",
        "Los boletines expedidos por la [pagina](https://presidente.gob.mx/) del presidente mexicano Andres Manuel Lopez Obrador forman el `corpus` usado a lo largo del documento, el cual fue procesado y limpiado en otra libreta. \n",
        "\n",
        "El EDA se realizara en la plataforma [Tableau](https://www.tableau.com/es-mx), para esto se guardaran periodicamente algunos valores en un archivo con extencion '.csv'  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FRT6snHHrZ"
      },
      "source": [
        "corpus = pd.read_json('../primera_iteracion/juve_datos.json',orient='record')\n",
        "\n",
        "\n",
        "corpus_validacion = corpus.sample(frac=8 / len(corpus) , random_state = 564)\n",
        "corpus_entrenamiento = corpus.drop(corpus_validacion.index)\n",
        "corpus_entrenamiento = corpus_entrenamiento.reset_index()\n",
        "\n",
        "print('Conjunto entrenamiento:')\n",
        "print(corpus_validacion.head())\n",
        "\n",
        "print('\\nConjunto validacion:')\n",
        "print(corpus_entrenamiento.head())\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "# Nota, este codigo esta por mientras el tokenizado. Borrar esto despues...\n",
        "stopword_es = nltk.corpus.stopwords.words('spanish')\n",
        "def remove_stopwords(text):\n",
        "    texto = text.split()\n",
        "    text = [word for word in texto if word not in stopword_es]\n",
        "    text_r = ' '.join(text)\n",
        "    return text_r\n",
        "corpus_entrenamiento['Titulo_ns'] = corpus_entrenamiento['Titulo'].apply(lambda x: remove_stopwords(x))\n",
        "#----------------------------------------------------------------------------"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conjunto entrenamiento:\n",
            "                                                 Titulo       Fecha\n",
            "2255  Poderes Ejecutivo y Judicial coordinan esfuerz...  13.02.2020\n",
            "2692  Presidente analiza terminar construcci\u00f3n de 54...  27.07.2019\n",
            "1929  Fotogaler\u00eda \u2013 Presidente inaugura Unidad de Me...  31.08.2020\n",
            "2291  Avanza adhesi\u00f3n de estados al Instituto de Sal...  28.01.2020\n",
            "1184  Reforma el\u00e9ctrica fortalecer\u00e1 a CFE y proteger...  23.10.2021\n",
            "\n",
            "Conjunto validacion:\n",
            "   index                                             Titulo       Fecha\n",
            "0      0  Presidente se re\u00fane en Palacio Nacional con go...  17.04.2023\n",
            "1      1  Obras complementarias en tramo 1 del Tren Maya...  17.04.2023\n",
            "2      2  Fonatur y Semarnat rinden informe de avances e...  17.04.2023\n",
            "3      3  En ruta del Tren Maya, estudiantes de todos lo...  17.04.2023\n",
            "4      4  Caminos pavimentados comunicar\u00e1n todas las cab...  17.04.2023\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "6848tskUzg"
      },
      "source": [
        "**H**ay que recalcar que los documentos no estan etiquetados, por lo que para el *topic modeling* se esta restringido a **algoritmos no supervisados**. Al algoritmo que se utiliza para conseguir la clasificacion de los topicos se le llamara **modelo**. El corpus de validacion es para realizar una evaluacion empirica de nuestro resultado. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "xRoYFFWRCb"
      },
      "source": [
        "## Vectorizacion\n",
        "---\n",
        "\n",
        "**E**l formato de texto suele ser poco amigable para los algoritmos por lo que es conveniente transformar el texto a un formato numerico que nos permita representar las **caracteristicas numericas** del texto. A esto se le llama vectorizado.\n",
        "\n",
        "La eleccion de las caracteristicas numericas depende en gran medida del modelo que se este usando. Los vectorizadores mas comunes en PLN son los basados en **frecuencias** y los basados en **algoritmos de aprendizaje**, los primeros nos permiten representar un documento por la frecuencia de sus palabras y se suele decir que los segundos logran capturar el lenguaje semantico de las palabras.    \n",
        "\n",
        "Intuitivamente se puede pensar que las palabras mas recurrentes representan a un documento, pero esto puede no ser verdad en los casos extremos donde una palabra aparece mucho en el corpus. Para resolver este problema existe el vectorizado *term frequency - inverse document frequency* `(tfidf)` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "9Cic7xJnPQ"
      },
      "source": [
        "frecuencias_del_corpus = TfidfVectorizer(\n",
        "    ngram_range = (1,2),\n",
        "    max_df=0.99999999999, \n",
        "    min_df=0\n",
        "    )\n",
        "\n",
        "corpus_vectorizado = frecuencias_del_corpus.fit_transform(corpus_entrenamiento.Titulo_ns) "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "4kYtPJPxsg"
      },
      "source": [
        "### Guardando frecuencias \n",
        "---\n",
        "**C**omo en la mayoria de modelos, se busca que nuestro *conjunto de entrenamiento* pueda capturar las caracteristicas **mas significativas** y que el tama\u00f1o del *vector caracteristicas* sea **lo mas peque\u00f1o posible**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "KcLUS7XcYx"
      },
      "source": [
        "print(frecuencias_del_corpus.stop_words_)\n",
        "\n",
        "matriz_cruda = corpus_vectorizado.toarray()\n",
        "tokenizador = frecuencias_del_corpus.build_analyzer()\n",
        "diccionario_vocabulario = frecuencias_del_corpus.vocabulary_\n",
        "\n",
        "archivo_documentos ='documentos.csv'\n",
        "archivo_diccionario='diccionario_token.csv'\n",
        "archivo_tokens_por_documento = 'palabras_por_documento.csv'\n",
        "archivo_matriz_cruda = 'matriz_cruda_frecuencias.csv'\n",
        "\n",
        "##################################################################################\n",
        "#Escribiendo el archivo 'informacion_documento' \n",
        "with open(archivo_documentos , 'w', encoding='utf-8') as archivo:\n",
        "    archivo.write(f'id_documento,titulo,fecha\\n')\n",
        "    for elemento in corpus_entrenamiento.iloc:\n",
        "        id_documento =elemento.name\n",
        "        titulo = elemento.Titulo\n",
        "        fecha = elemento.Fecha\n",
        "        archivo.write(f'{id_documento},{titulo},{fecha}\\n')\n",
        "\n",
        "        \n",
        "#_________________________________________________________________________________\n",
        "\n",
        "\n",
        "##################################################################################'\n",
        "# Guardando diccionario de los tokens\n",
        "with open(archivo_diccionario, 'w', encoding='utf-8') as archivo:\n",
        "    archivo.write(f'id_token,token\\n')\n",
        "    for (llave) in diccionario_vocabulario:\n",
        "        archivo.write(f'{llave},{diccionario_vocabulario[llave]}\\n')\n",
        "#_________________________________________________________________________________ \n",
        "\n",
        "\n",
        "##################################################################################\n",
        "#Escribiendo el archivo 'palabras_por_documento' \n",
        "with open(archivo_tokens_por_documento, 'w', encoding='utf-8') as archivo:\n",
        "    archivo.write(f'id_documento,token,frecuencia\\n')\n",
        "    for elemento in corpus_entrenamiento.iloc:\n",
        "        id_documento =elemento.name\n",
        "        for token in (tokenizador(elemento.Titulo_ns)):\n",
        "            id_token =diccionario_vocabulario[token] \n",
        "            frecuencia = matriz_cruda[id_documento][id_token]\n",
        "            archivo.write(f'{id_documento},{token},{frecuencia}\\n')\n",
        "#_________________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "#Escribiendo el archivo 'matriz_cruda_frecuencias'\n",
        "dataframe_corpus = pd.DataFrame(corpus_vectorizado.toarray())\n",
        "dataframe_corpus.to_csv(archivo_matriz_cruda,index=False)\n",
        "#Ejemplo_leer = pd.read_csv('frecuencias_del_corpus.csv')\n",
        "#_________________________________________________________________________________ \n",
        "\n",
        "    \n",
        "print('listo')"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set()\n",
            "listo\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "n49ynRcYED"
      },
      "source": [
        "## Reduciendo dimencionalidad\n",
        "---\n",
        "\n",
        "**C**uando el vector de caracteristicas es muy grande se suele sufrir de *la maldici\u00f3n de la dimensionalidad* (o efecto Hughes), el cual es un problema conocido de la estadistica y probabilidadEn. Para mitigar estos efectos la solucion inmediata es reducir el numero de caracteristicas en nuestro vector. Los metodos mas populares son el **PCA** y usando una distribucion **chi2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "fU5YKApzfJ"
      },
      "source": [
        "#If 0 < n_components < 1 and svd_solver == 'full', \n",
        "#select the number of components such that the amount of variance that \n",
        "#needs to be explained is greater than the percentage specified by n_components.\n",
        "#svd_solver{\u2018auto\u2019, \u2018full\u2019, \u2018arpack\u2019, \u2018randomized\u2019}\n",
        "\n",
        "pca_corpus_entrenamiento = PCA(\n",
        "    n_components= 0.70,\n",
        "    svd_solver= 'full',\n",
        "    whiten = False,\n",
        "    random_state=576\n",
        "    ) \n",
        "pca_corpus_entrenamiento.fit(corpus_vectorizado.toarray())\n",
        "corpus_frecuencias_reducidas = corpus_frecuencias_reducidas.transform(corpus_vectorizado)\n",
        "\n",
        "print('listo')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "iu60THlJkm"
      },
      "source": [
        "print('f')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "AadVqJaR3O"
      },
      "source": [
        "print(corpus_frecuencias_reducidas)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "5LPkPSAVCf"
      },
      "source": [
        "### Guardando nuevas caracteristicas\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "oGwWYSkim7"
      },
      "source": [
        "## Aplicando Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "qdEBV5XB4t"
      },
      "source": [
        "**L**as tecnicas de analizar grupos en aprendizaje automatico se conocen como **clustering** y el algoritmo mas popular de clustering es **K-means** el cual usa un enfoque basado en la *distancia* entre vectores de caracteristicas. \n",
        "\n",
        "Sin embargo para agrupar documentos existe un metodo muy popular que consiste en calcular dos *distribuciones de probabilidades*, Una de *topicos sobre tokens* y otra de *documentos sobre topicos*. Al algoritmo que calcula estas dos distribuciones se le llama **Latent Dirichlet Allocation**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "1diuA1iQXC"
      },
      "source": [
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_components,\n",
        "    max_iter=500,\n",
        "    learning_method=\"online\",\n",
        "    learning_offset=50.0,\n",
        "    random_state=876,\n",
        ")\n",
        "lda.fit()\n",
        "\n",
        "feature = frecuencias_del_corpus.get_feature_names()\n",
        "for ind, topic in enumerate(lda.components_):\n",
        "    print('Top 50 words in topic {}'.format(ind))\n",
        "    print('-'*25)\n",
        "    topic_ordenado = topic.argsort()[::-1]\n",
        "    top_50 = topic_ordenado[:50]\n",
        "    print([feature[i] for i in top_50] , '\\n\\n')"
      ],
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'n_components' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10828\\4262063987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m lda = LatentDirichletAllocation(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlearning_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"online\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlearning_offset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'n_components' is not defined"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "3z1qUUIUa2"
      },
      "source": [
        "### Guardando entropia del topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "dmoDawzJbG"
      },
      "source": [
        "## Evaluacion empirica\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "UBVtbKkiJT"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "hbbFE7hH5d"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
